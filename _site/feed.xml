<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Yancey&#39;s Personal Website</title>
    <description>Yancey&#39;s blog</description>
    <link>/</link>
    <atom:link href="/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Mon, 27 Jun 2016 19:19:07 +0800</pubDate>
    <lastBuildDate>Mon, 27 Jun 2016 19:19:07 +0800</lastBuildDate>
    <generator>Jekyll v3.1.2</generator>
    
      <item>
        <title>Kubernetes Service Network Explain</title>
        <description>&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#service&quot; id=&quot;markdown-toc-service&quot;&gt;Service&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#clusterip--nodeport--loadbalancer&quot; id=&quot;markdown-toc-clusterip--nodeport--loadbalancer&quot;&gt;ClusterIP &amp;amp;&amp;amp; NodePort &amp;amp;&amp;amp; LoadBalancer&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#clusterip&quot; id=&quot;markdown-toc-clusterip&quot;&gt;1. ClusterIP&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#rcreplices-count3&quot; id=&quot;markdown-toc-rcreplices-count3&quot;&gt;1.1 发布一个rc，并指定replices count为3.&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#service1label&quot; id=&quot;markdown-toc-service1label&quot;&gt;1.2. 发布一个service，并指定步骤1中的label。&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#iptablesnatservice&quot; id=&quot;markdown-toc-iptablesnatservice&quot;&gt;1.3. 查看iptables，观察其NAT表中的信息（只截取了部分和这个service有关的信息）&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#loadbalancer&quot; id=&quot;markdown-toc-loadbalancer&quot;&gt;3. LoadBalancer&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#section&quot; id=&quot;markdown-toc-section&quot;&gt;服务发现&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#section-1&quot; id=&quot;markdown-toc-section-1&quot;&gt;附录&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;service&quot;&gt;Service&lt;/h2&gt;
&lt;p&gt;Pod的IP是在&lt;strong&gt;docker0&lt;/strong&gt;网段动态分配的，当发生重启，扩容等操作时，IP地址会随之变化。当某个Pod(frontend)需要去访问其依赖的另外一组Pod(backend)时，如果backend的IP发生变化时，如何保证fronted到backend的正常通信变的非常重要。由此，引出了Service的概念。&lt;/p&gt;

&lt;p&gt;这里docker0是一个网桥，docker daemon启动container时会根据docker0的网段来划粉container的IP地址&lt;a href=&quot;https://docs.docker.com/engine/userguide/networking/dockernetworks/&quot;&gt;Docker网络&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;在实际生产环境中，对Service的访问可能会有两种来源：Kubernetes集群内部的程序（Pod）和Kubernetes集群外部，为了满足上述的场景，Kubernetes service有以下三种类型：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;ClusterIP&lt;/strong&gt;:提供一个集群内部的虚拟IP以供Pod访问。&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;NodePort&lt;/strong&gt;:在每个Node上打开一个端口以供外部访问。&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;LoadBalancer&lt;/strong&gt;:通过外部的负载均衡器来访问。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;clusterip--nodeport--loadbalancer&quot;&gt;ClusterIP &amp;amp;&amp;amp; NodePort &amp;amp;&amp;amp; LoadBalancer&lt;/h2&gt;

&lt;h3 id=&quot;clusterip&quot;&gt;1. ClusterIP&lt;/h3&gt;

&lt;p&gt;此模式会提供一个集群内部的虚拟IP（与Pod不在同一网段)，以供集群内部的pod之间通信使用。
ClusterIP也是Kubernetes service的默认类型。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;service-network.png&quot; alt=&quot;Pod-Service&quot; /&gt;&lt;/p&gt;

&lt;p&gt;为了实现图上的功能主要需要以下几个组件的协同工作&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;apiserver&lt;/strong&gt; 用户通过kubectl命令向apiserver发送创建service的命令，apiserver接收到请求以后将数据存储到etcd中。&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;kube-proxy&lt;/strong&gt; kubernetes的每个节点中都有一个叫做kube-proxy的进程，这个进程负责感知service，pod的变化，并将变化的信息写入本地的iptables中。&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;iptables&lt;/strong&gt; 使用NAT等技术将virtualIP的流量转至endpoint中。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;下面我们实际发布一个Service，能够更清晰的了解到Service是如何工作的。&lt;/p&gt;

&lt;h4 id=&quot;rcreplices-count3&quot;&gt;1.1 发布一个rc，并指定replices count为3.&lt;/h4&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;yancey@ yancey-macbook kubernetes-1$kubectl create -f rc_nginx.yaml
yancey@ yancey-macbook kubernetes-1$kubectl get pods
NAME                     READY     STATUS    RESTARTS   AGE
k8s-master-core-v2-01    4/4       Running   0          8m
k8s-proxy-core-v2-01     1/1       Running   0          8m
nginx-controller-6bovu   1/1       Running   0          4m
nginx-controller-iowux   1/1       Running   0          4m
nginx-controller-o7m6u   1/1       Running   0          4m
yancey@ yancey-macbook kubernetes-1$kubectl describe pod nginx-controller-6bovu
Name:		nginx-controller-6bovu
Namespace:	default
Node:		core-v2-01/172.17.8.101
Start Time:	Fri, 17 Jun 2016 15:22:20 +0800
Labels:		app=nginx
Status:		Running
IP:		10.1.13.3
Controllers:	ReplicationController/nginx-controller
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h4 id=&quot;service1label&quot;&gt;1.2. 发布一个service，并指定步骤1中的label。&lt;/h4&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;yancey@ yancey-macbook kubernetes-1$kubectl create -f service_nginx.yaml
yancey@ yancey-macbook kubernetes-1$kubectl get svc
NAME            CLUSTER-IP   EXTERNAL-IP   PORT(S)    AGE
kubernetes      10.0.0.1     &amp;lt;none&amp;gt;        443/TCP    9m
nginx-service   10.0.0.252   &amp;lt;none&amp;gt;        8000/TCP   4m
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;kubernetes为nginx-server这个service创建了一个10.0.0.252这个ClusterIP，也可以称为VirtualIP.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;yancey@ yancey-macbook kubernetes-1$kubectl get ep
NAME            ENDPOINTS                                AGE
kubernetes      172.17.8.101:6443                        9m
nginx-service   10.1.13.2:80,10.1.13.3:80,10.1.13.4:80   4m
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;查看endpoint信息，发现nginx-service一共有三个endpoint地址，分别对应nginx的三个pod。&lt;/p&gt;

&lt;h4 id=&quot;iptablesnatservice&quot;&gt;1.3. 查看iptables，观察其NAT表中的信息（只截取了部分和这个service有关的信息）&lt;/h4&gt;
&lt;p&gt;查看iptables中NAT表的命令:&lt;code class=&quot;highlighter-rouge&quot;&gt;iptables -L -v -n -t nat&lt;/code&gt;&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Chain PREROUTING (policy ACCEPT 0 packets, 0 bytes)
 pkts bytes target     prot opt in     out     source               destination
   37  2766 KUBE-SERVICES  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* kubernetes service portals */
   33  2112 DOCKER     all  --  *      *       0.0.0.0/0            0.0.0.0/0            ADDRTYPE match dst-type LOCAL
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;在PREROUTING链中会先匹配到KUBE-SERVICES这个Chain。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Chain KUBE-SERVICES (2 references)
 pkts bytes target     prot opt in     out     source               destination
    0     0 KUBE-SVC-GKN7Y2BSGW4NJTYL  tcp  --  *      *       0.0.0.0/0            10.0.0.252           /* default/nginx-service: cluster IP */ tcp dpt:8000
   18  1080 KUBE-NODEPORTS  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* kubernetes service nodeports; NOTE: this must be the last rule in this chain */ ADDRTYPE match dst-type LOCAL

&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;所有destinationIP为10.0.0.252的包都转到&lt;strong&gt;KUBE-SVC-GKN7Y2BSGW4NJTYL&lt;/strong&gt;这个Chain&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Chain KUBE-SVC-GKN7Y2BSGW4NJTYL (1 references)
 pkts bytes target     prot opt in     out     source               destination
    0     0 KUBE-SEP-7ROBBXFV7SD4AIRW  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* default/nginx-service: */ statistic mode random probability 0.33332999982
    0     0 KUBE-SEP-XY3F6VJIZ7ELIF4Z  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* default/nginx-service: */ statistic mode random probability 0.50000000000
    0     0 KUBE-SEP-JIDZHFC4A3T535AK  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* default/nginx-service: */
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;这里能看到请求会平均执行&lt;strong&gt;KUBE-SEP-7ROBBXFV7SD4AIRW&lt;/strong&gt;,&lt;strong&gt;KUBE-SEP-XY3F6VJIZ7ELIF4Z&lt;/strong&gt;,&lt;strong&gt;KUBE-SEP-XY3F6VJIZ7ELIF4Z&lt;/strong&gt;这三个Chain.最后我们看下&lt;strong&gt;KUBE-SEP-7ROBBXFV7SD4AIRW&lt;/strong&gt;这个Chain做了什么&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Chain KUBE-SEP-7ROBBXFV7SD4AIRW (1 references)
 pkts bytes target     prot opt in     out     source               destination
    0     0 KUBE-MARK-MASQ  all  --  *      *       10.1.13.2            0.0.0.0/0            /* default/nginx-service: */
    0     0 DNAT       tcp  --  *      *       0.0.0.0/0            0.0.0.0/0            /* default/nginx-service: */ tcp to:10.1.13.2:80

&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;这个Chain使用了DNAT规则将流量转发到10.1.13.2:80这个地址。至此从Pod发出来的流量通过本地的iptables将流量转至了service背后的pod上。
### 2. NodePort
Kubernetes将会在每个Node上打开一个端口并且&lt;strong&gt;每个Node的端口都是一样的&lt;/strong&gt;，通过&amp;lt;NodeIP&amp;gt;:NodePort的方式Kubernetes集群外部的程序可以访问Service。&lt;/p&gt;

&lt;p&gt;修改kubernetes/service_nginx.yaml，将Service的type改为NodePort类型。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;apiVersion: v1
kind: Service
metadata:
  name: nginx-service
spec:
  type: NodePort
  ports:
  - port: 8000 
    targetPort: 80
    protocol: TCP
  # just like the selector in the replication controller,
  # but this time it identifies the set of pods to load balance
  # traffic to.
  selector:
    app: nginx
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;发布这个service，可以看到已经随机分配了一个NodePort端口。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;yancey@ yancey-macbook kubernetes-1$kubectl get svc nginx-service -o yaml
apiVersion: v1
kind: Service
metadata:
  creationTimestamp: 2016-06-18T03:29:42Z
  name: nginx-service
  namespace: default
  resourceVersion: &quot;1405&quot;
  selfLink: /api/v1/namespaces/default/services/nginx-service
  uid: e50ba23a-3504-11e6-a94f-080027a75e9e
spec:
  clusterIP: 10.0.0.229
  ports:
  - nodePort: 30802
    port: 8000
    protocol: TCP
    targetPort: 80
  selector:
    app: nginx
  sessionAffinity: None
  type: NodePort
status:
  loadBalancer: {}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;观察Iptables中的变化，KUBE-NODEPORTS这个Chain中增加了如下内容&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Chain KUBE-NODEPORTS (1 references)
 pkts bytes target     prot opt in     out     source               destination
    0     0 KUBE-MARK-MASQ  tcp  --  *      *       0.0.0.0/0            0.0.0.0/0            /* default/nginx-service: */ tcp dpt:30802
    0     0 KUBE-SVC-GKN7Y2BSGW4NJTYL  tcp  --  *      *       0.0.0.0/0            0.0.0.0/0            /* default/nginx-service: */ tcp dpt:30802
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;可以看到流量会转至KUBE-SVC-GKN7Y2BSGW4NJTYL这个Chain中处理一次，也就是ClusterIP中提到的通过负载均衡将流量平均分配到3个endpoint上。&lt;/p&gt;

&lt;h3 id=&quot;loadbalancer&quot;&gt;3. LoadBalancer&lt;/h3&gt;
&lt;p&gt;待补充。&lt;/p&gt;

&lt;h3 id=&quot;section&quot;&gt;服务发现&lt;/h3&gt;
&lt;p&gt;当发布一个服务之后，我们要使用这个服务，第一个问题就是要拿到这些服务的IP和PORT，kubernetes提供两种方式以便在程序中去动态的获取这些信息。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;ENV环境变量
在Pod其中之后，kubernetes会将现有服务的IP，PORT以环境变量的方式写入pod中，程序只要读取这些环境变量即可。&lt;/li&gt;
  &lt;li&gt;DNS，程序中可以使用server的名称对服务进行访问，在程序启时候，不必预先读取环境变量中的内容。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;这是两种方式的详细说明：&lt;a href=&quot;http://kubernetes.io/docs/user-guide/services/#discovering-services&quot;&gt;http://kubernetes.io/docs/user-guide/services/#discovering-services&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;section-1&quot;&gt;附录&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/4/html/Security_Guide/s1-fireall-ipt-act.html#s2-firewall-policies&quot;&gt;iptables文档&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://kubernetes.io/docs/user-guide/debugging-services/&quot;&gt;Debuging Service&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Tue, 21 Jun 2016 00:09:00 +0800</pubDate>
        <link>/blog/2016/06/21/k8s-service-network</link>
        <guid isPermaLink="true">/blog/2016/06/21/k8s-service-network</guid>
        
        
        <category>k8s</category>
        
      </item>
    
      <item>
        <title>Linux IO多路复用</title>
        <description>&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#ioselectpollepoll&quot; id=&quot;markdown-toc-ioselectpollepoll&quot;&gt;IO多路复用之select，poll，epoll对比&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#poll&quot; id=&quot;markdown-toc-poll&quot;&gt;2. poll&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#epoll&quot; id=&quot;markdown-toc-epoll&quot;&gt;3. epoll&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#level-triggeredlt&quot; id=&quot;markdown-toc-level-triggeredlt&quot;&gt;3.1 水平触发(Level-triggered，LT)&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#section&quot; id=&quot;markdown-toc-section&quot;&gt;4. 总结&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;ioselectpollepoll&quot;&gt;IO多路复用之select，poll，epoll对比&lt;/h2&gt;
&lt;p&gt;### 1. select
* 受限于文件打开数，FD_SETSIZE来控制，通过cat /proc/sys/fs/file-max来查看
* 对socket列表轮询，如果就绪socket很少，会造成大量的浪费。&lt;/p&gt;

&lt;h3 id=&quot;poll&quot;&gt;2. poll&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;从select升级而来，链表存储，理论上支持的连接数无上限。&lt;/li&gt;
  &lt;li&gt;“水平触发”机制,如果报了fd没有被处理，下次会继续报告此fd。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;epoll&quot;&gt;3. epoll&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;没有fd数量限制&lt;/li&gt;
  &lt;li&gt;存储在内核的事件表中，基于fd的callback。&lt;/li&gt;
  &lt;li&gt;支持”水平触发”，”边缘触发”&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;level-triggeredlt&quot;&gt;3.1 水平触发(Level-triggered，LT)&lt;/h4&gt;
&lt;p&gt;内核会通知fd是否就绪，如果不处理则下次会继续通知。
#### 3.2 边缘触发(edge-triggered，ET)
只支持no-block socket，当fd就绪时内核会通知进程，并且只会通知一次，是一种高效的处理方式。&lt;/p&gt;

&lt;h3 id=&quot;section&quot;&gt;4. 总结&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;通常情况下epoll性能会好一些，但如果在大量活跃连接情况下，epoll由于存在大量回调，性能未必会比select或poll差。&lt;/li&gt;
&lt;/ul&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt; &lt;/th&gt;
      &lt;th&gt;最大连接数&lt;/th&gt;
      &lt;th&gt;fd增加后的性能问题&lt;/th&gt;
      &lt;th&gt;消息传递&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;select&lt;/td&gt;
      &lt;td&gt;受限于最大fd打开数，FD_SETSIZE&lt;/td&gt;
      &lt;td&gt;遍历方式，fd打开数增加会导致性能下降&lt;/td&gt;
      &lt;td&gt;内核到用户空间的拷贝&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;poll&lt;/td&gt;
      &lt;td&gt;基于链表，理论上无最大fd打开数的限制&lt;/td&gt;
      &lt;td&gt;同上&lt;/td&gt;
      &lt;td&gt;同上&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;epoll&lt;/td&gt;
      &lt;td&gt;很大，1G内存可以打开10w左右连接&lt;/td&gt;
      &lt;td&gt;基于fd的callback实现，活跃的fd才会调用callback，活跃少的情况下无性能问题&lt;/td&gt;
      &lt;td&gt;基于mmap实现，共享一块内存&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
</description>
        <pubDate>Mon, 02 May 2016 01:07:00 +0800</pubDate>
        <link>/blog/2016/05/02/linux-io</link>
        <guid isPermaLink="true">/blog/2016/05/02/linux-io</guid>
        
        
        <category>linux</category>
        
      </item>
    
      <item>
        <title>【Linux学习笔记】之进程管理</title>
        <description>&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#section&quot; id=&quot;markdown-toc-section&quot;&gt;1. 进程地址空间：虚拟地址空间&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#namespace&quot; id=&quot;markdown-toc-namespace&quot;&gt;1.2 namespace:&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#namespace-1&quot; id=&quot;markdown-toc-namespace-1&quot;&gt;1.3 6种namespace:&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#section-1&quot; id=&quot;markdown-toc-section-1&quot;&gt;1.4 进程的复制：&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#cow&quot; id=&quot;markdown-toc-cow&quot;&gt;1.5 COW技术：&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#section-2&quot; id=&quot;markdown-toc-section-2&quot;&gt;2. 进程调度&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#section-3&quot; id=&quot;markdown-toc-section-3&quot;&gt;2.1 完全公平调度器&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section&quot;&gt;1. 进程地址空间：虚拟地址空间&lt;/h2&gt;
&lt;p&gt;与实际物理地址无关。虚拟地址空间分为：内核空间+用户空间（TASK_SIZE）&lt;/p&gt;

&lt;p&gt;使用页表为物理地址分配虚拟地址，内核负责将虚拟地址空间映射到物理地址空间，所以内核决定了哪些可以共享，哪些不可以共享。
使用的模型：多级分页&lt;/p&gt;

&lt;p&gt;物理内存的分配:&lt;strong&gt;伙伴系统&lt;/strong&gt;,目的：快速检测连续区域&lt;/p&gt;

&lt;p&gt;进程的生命周期：运行、等待、睡眠&lt;/p&gt;

&lt;p&gt;查看限制信息：cat /proc/self/limits&lt;/p&gt;

&lt;p&gt;进程的两种启动方式：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;fork：当前进程的一个副本&lt;/li&gt;
  &lt;li&gt;exec：从一个可执行的二进制文件加载另一个app&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;线程的启动方式：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;clone：功能强大，可以控制子进程与父进程的数据共享粒度。可以创建线程，可以使用独立的namespace&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;namespace&quot;&gt;1.2 namespace:&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;通过fork或clone可以控制是否与父进程共享命名空间&lt;/li&gt;
  &lt;li&gt;通过unshare将进程某些部分从父进程分离，包括namespace&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;namespace-1&quot;&gt;1.3 6种namespace:&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;UTS(Unix Timesharing System,主机名),IPC（进程通信），PID（chroot进程树），NS（挂载点），NET（网络），USER&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;section-1&quot;&gt;1.4 进程的复制：&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;fork:完整副本，COW技术&lt;/li&gt;
  &lt;li&gt;vfork:父子进程共享数据（由于fork的cow技术，vfork不被广泛采用）&lt;/li&gt;
  &lt;li&gt;clone：产生线程，可以对父子进程之间的共享、复制进行精确控制。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;cow&quot;&gt;1.5 COW技术：&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;执行fork时只负责其页表，而不是复制整个地址空间&lt;/li&gt;
  &lt;li&gt;涉及写操作的部分，会新申请空间，更新页表&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-2&quot;&gt;2. 进程调度&lt;/h2&gt;

&lt;h3 id=&quot;section-3&quot;&gt;2.1 完全公平调度器&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;虚拟时钟，完全公平调度器依赖虚拟时钟，度量等待进程在完全公平系统中所能得到的CPU时间。虚拟时钟的实现方法是根据现实的时钟以及负载情况推算出来的。&lt;/li&gt;
  &lt;li&gt;实现方式：基于红黑树，vruntime作为key，每次选择最左端节点来执行&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Thu, 14 Apr 2016 01:15:00 +0800</pubDate>
        <link>/blog/2016/04/14/linux-kernal-process-manage</link>
        <guid isPermaLink="true">/blog/2016/04/14/linux-kernal-process-manage</guid>
        
        
        <category>linux</category>
        
      </item>
    
      <item>
        <title>Redis Distributed Lock</title>
        <description>&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#section&quot; id=&quot;markdown-toc-section&quot;&gt;1.我们先了解一下什么是分布式以及它的应用场景&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#redis&quot; id=&quot;markdown-toc-redis&quot;&gt;2.如何借助Redis实现一个分布式锁&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#section-1&quot; id=&quot;markdown-toc-section-1&quot;&gt;2.1. 三个原则&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#section-2&quot; id=&quot;markdown-toc-section-2&quot;&gt;2.2. 几种实现方式&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#section-3&quot; id=&quot;markdown-toc-section-3&quot;&gt;2.2.1.我们先考虑一种简单的实现方式&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#section-4&quot; id=&quot;markdown-toc-section-4&quot;&gt;2.2.2. 单节点下的一种正确实现方法&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#redlock&quot; id=&quot;markdown-toc-redlock&quot;&gt;2.2.3. Redlock算法&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#section-5&quot; id=&quot;markdown-toc-section-5&quot;&gt;2.2.4 失败后的重试操作&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#section-6&quot; id=&quot;markdown-toc-section-6&quot;&gt;2.2.5 释放锁&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#section-7&quot; id=&quot;markdown-toc-section-7&quot;&gt;3.回顾&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#section-8&quot; id=&quot;markdown-toc-section-8&quot;&gt;4.参考资料&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section&quot;&gt;1.我们先了解一下什么是分布式以及它的应用场景&lt;/h2&gt;

&lt;p&gt;当多个不同进程互斥的方式访问某个共享资源时需要使用到分布式锁来保证各进程安全的访问数据。&lt;/p&gt;

&lt;h2 id=&quot;redis&quot;&gt;2.如何借助Redis实现一个分布式锁&lt;/h2&gt;

&lt;h3 id=&quot;section-1&quot;&gt;2.1. 三个原则&lt;/h3&gt;

&lt;p&gt;安全性：互斥，在任何时候只有一个客户端可以获取到锁&lt;/p&gt;

&lt;p&gt;存活性 A:无死锁，无论客户端是否crash掉，总可以获取到一个锁&lt;/p&gt;

&lt;p&gt;存活性 B:容错，只要Redis的大部分节点是可用的，客户端就可以获取或者释放锁&lt;/p&gt;

&lt;h3 id=&quot;section-2&quot;&gt;2.2. 几种实现方式&lt;/h3&gt;

&lt;h4 id=&quot;section-3&quot;&gt;2.2.1.我们先考虑一种简单的实现方式&lt;/h4&gt;
&lt;p&gt;依赖redis中过期时间的机制，当获取锁时在实例中创建一个key，释放时候进行删除操作
大多数情况下可以正常运行，但这是一个单点，当实例挂掉怎么办呢？是的挂一个slave上去！可这并不是一个好方案，因为redis的备份是异步进行，这无法满足我们的安全性，那么为什么呢？
这个模型中有个明显的竞态条件&lt;/p&gt;

&lt;p&gt;1）Client A在master中获取到了锁&lt;/p&gt;

&lt;p&gt;2）master在将key同步到slave之前挂掉了&lt;/p&gt;

&lt;p&gt;3）slave升级为了master&lt;/p&gt;

&lt;p&gt;4）Client B通过原来的slave的节点获取同样的资源A，违反了安全性。&lt;/p&gt;

&lt;p&gt;那我们如何去改进这一做法呢？&lt;/p&gt;

&lt;h4 id=&quot;section-4&quot;&gt;2.2.2. 单节点下的一种正确实现方法&lt;/h4&gt;

&lt;p&gt;获取锁资源&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    SET resource_name my_random_value NX PX 30000
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;解释一下这个命令,写入一个key为resource_name,value为随机值
，NX表示只有在key存在的情况下才会写入数据，PX表示过期时间的单位为毫秒，设置的过期时间
为30000毫秒。value必须为唯一值，只有这样客户端才可以安全的方式释放一个锁。例如：
只有在key存在，并且value是我之前写入的那个值时才会删除这个key。Lua脚本如下：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;if redis.call(&quot;get&quot;,KEYS[1]) == ARGV[1] then
    return redis.call(&quot;del&quot;,KEYS[1])
else
    return 0
end
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;如何阻止一个客户端删除一个不是它创建的锁是非常重要的。例如一个获取到了一个锁资源，
但是在被某个操作阻塞住了，直到超了锁的有效时间也就是Redis中的过期时间。然后呢尝试去
释放这个锁，但这个资源已经被其他客户端占用了。简单的使用DEL操作会导致删除其他客户端
写入的key，这是非常不安全的。像上面脚本描述的那样，为每个key设计一个”签名”，这样
这个锁只会被写入的那个客户端所删除。&lt;/p&gt;

&lt;p&gt;到这里，我们有了一个安全的获取和释放锁资源的方法。这个系统是假定在一个非分布式，
单点，总是可用的一个系统，那么我们将这些概念扩展到一个分布式系统上来。&lt;/p&gt;

&lt;h4 id=&quot;redlock&quot;&gt;2.2.3. Redlock算法&lt;/h4&gt;
&lt;p&gt;在分布式版本的算法中，我们假设有N个redis-master节点。所有的节点是独立的，我们已经描述
了如何在单机redis下释放和获取锁资源。在我们的例子中，另N=5，这是个有意义的值，
能够保证在大多数独立的情况下使他们出现故障。&lt;/p&gt;

&lt;p&gt;为了获取锁，客户端会执行一下步骤&lt;/p&gt;

&lt;p&gt;1）获取当前的毫秒级时间&lt;/p&gt;

&lt;p&gt;2）客户端尝试顺序的在所有N个redis节点上获取锁，使用相同的key和随机value。
在步骤二的过程中客户端户会设置一个比所有自动释放小的超时时间以便能够获取到锁。
例如：自动释放时间设置为10s，超时时间设置为5-50ms,如果一个实例不可用，我们可以尽快去尝试其他实例而避免超时。&lt;/p&gt;

&lt;p&gt;3）客户端为了获得锁会计算从第一步开始花费的时间，当且仅当客户端在大多数节点获得到了锁（最少三个节点），并且总共的花费时间小于锁的存活时间，锁才会被这个客户端获取到。&lt;/p&gt;

&lt;p&gt;4）一但客户端获取到了锁，它的存活时间为初始化时间减去步骤3花费的时间&lt;/p&gt;

&lt;p&gt;5）如果客户端因为某种原因获取锁的过程中失败了，例如没有在存活时间内在N/2+1个节点以上获取到锁资源，它会尝试在所有的节点上释放锁资源。&lt;/p&gt;

&lt;h4 id=&quot;section-5&quot;&gt;2.2.4 失败后的重试操作&lt;/h4&gt;
&lt;p&gt;如果一个客户端没成功的获取到锁，它会在随机的一段时间后重新尝试获取锁资源。当然客户端尝试的频率越快，空窗期越小，但这样会有可能多个客户端同时尝试获取锁资源，同时获取不到的情况，这时就产生了”脑裂”的现象。所以最理想的情况是客户端用复用技术同时像N个节点发送SET命令。&lt;/p&gt;

&lt;p&gt;值得注意的一点，当客户端在大多数节点获取锁时失败了，需要及时的释放掉已获取锁的节点上的key，如果不能及时释放就只能等到过期时间到了其他客户端才可以在这个节点上获取锁了。&lt;/p&gt;

&lt;h4 id=&quot;section-6&quot;&gt;2.2.5 释放锁&lt;/h4&gt;
&lt;p&gt;释放操作比较简单，只需要在所有的节点上执行一遍释放操作，而不管客户端是否在这个节点上获取到了一个锁。&lt;/p&gt;

&lt;h2 id=&quot;section-7&quot;&gt;3.回顾&lt;/h2&gt;
&lt;p&gt;我们简单回顾下上面介绍的内容&lt;/p&gt;

&lt;p&gt;3.1 介绍了什么是分布式锁以及应用场景&lt;/p&gt;

&lt;p&gt;3.2 介绍了分不是锁的一种简单实现，但遇到了安全性的问题&lt;/p&gt;

&lt;p&gt;3.3 使用”签名”的机制来保证安全性，说明了使用单节点redis来解决分布式锁的问题。&lt;/p&gt;

&lt;p&gt;3.4 根据Redlock算法使用redis集群解决分布式锁的问题&lt;/p&gt;

&lt;h2 id=&quot;section-8&quot;&gt;4.参考资料&lt;/h2&gt;

&lt;p&gt;http://redis.io/topics/distlock&lt;/p&gt;

&lt;p&gt;http://martin.kleppmann.com/2016/02/08/how-to-do-distributed-locking.html&lt;/p&gt;

</description>
        <pubDate>Tue, 29 Mar 2016 17:35:21 +0800</pubDate>
        <link>/blog/2016/03/29/redis-distributed-lock</link>
        <guid isPermaLink="true">/blog/2016/03/29/redis-distributed-lock</guid>
        
        
        <category>redis</category>
        
      </item>
    
  </channel>
</rss>
